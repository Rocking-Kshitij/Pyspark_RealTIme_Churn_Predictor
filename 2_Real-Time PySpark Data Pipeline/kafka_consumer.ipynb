{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5023809b-673a-4cac-bda1-95ba4580e436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, TimestampType\n",
    "\n",
    "spark.conf.set(\"fs.s3a.access.key\", \"XXX\")\n",
    "spark.conf.set(\"fs.s3a.secret.key\", \"XXX\")\n",
    "spark.conf.set(\"fs.s3a.endpoint\", \"s3.us-east-1.amazonaws.com\")\n",
    "# df = spark.read.format(\"parquet\").load(\"s3a://mlops-clean-data-bucket/path/customer_features/\")\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8340e2d9-a428-4db9-896c-273e7f9c23c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define schema\n",
    "schema = StructType() \\\n",
    "    .add(\"customer_id\", StringType()) \\\n",
    "    .add(\"event\", StringType()) \\\n",
    "    .add(\"timestamp\", TimestampType()) \\\n",
    "    .add(\"product_id\", StringType()) \\\n",
    "    .add(\"amount\", DoubleType())\n",
    "\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"54.160.8.148:9092\") \\\n",
    "    .option(\"subscribe\", \"customer_events\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Extract value (in JSON string format)\n",
    "json_df = kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse JSON to columns\n",
    "parsed_df = json_df.select(from_json(col(\"value\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "parsed_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70551b1d-9d3a-4e71-a7f3-df7d4f61e177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, avg, window, to_utc_timestamp\n",
    "\n",
    "features_df = parsed_df \\\n",
    "    .withColumnRenamed(\"timestamp\", \"event_time\") \\\n",
    "    .withWatermark(\"event_time\", \"1 minutes\") \\\n",
    "    .groupBy(\n",
    "        col(\"customer_id\"),\n",
    "        window(col(\"event_time\"), \"30 second\")\n",
    "        \n",
    "    ).agg(\n",
    "        count(\"event\").alias(\"total_events\"),\n",
    "        avg(\"amount\").alias(\"avg_amount\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98133333-c53d-43e4-990c-e90bfa546a5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# use when required\n",
    "# parsed_df.writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .option(\"truncate\", False) \\\n",
    "#     .option(\"checkpointLocation\", \"/tmp/spark_checkpoints\") \\\n",
    "#     .start()\n",
    "# use when required\n",
    "features_df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"s3a://mlops-clean-data-bucket/customer_features/\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/feature-stream/\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='2 minutes') \\\n",
    "    .queryName(\"features_stream\") \\\n",
    "    .start()\n",
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f1a2747-3f65-4a94-9e19-5800eb3f076c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extras\n",
    "#--------removefolder --------\n",
    "# dbutils.fs.rm(\"dbfs:/tmp/checkpoints/feature-stream\", True)\n",
    "#-------show---------\n",
    "# df = spark.read.format(\"parquet\").load(\"s3a://mlops-clean-data-bucket/path/customer_features/\")\n",
    "# df.show()\n",
    "#----count------------\n",
    "# df = spark.read.format(\"parquet\").load(\"s3a://mlops-clean-data-bucket/customer_features/\")\n",
    "# print(\"Row count:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2b9cd3c-f413-41c5-8037-5d578f0af6b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "kafka_consumer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
